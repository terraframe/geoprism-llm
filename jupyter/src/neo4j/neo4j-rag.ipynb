{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "MODEL = 'llama3.2'\n",
    "#MODEL = 'firefunction-v2'\n",
    "#MODEL = 'mistral-nemo'\n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=MODEL,\n",
    "    temperature=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "    names: List[str] = Field(\n",
    "        description=\"All the entities that appear in the text\", default_factory=list)\n",
    "    \n",
    "prompt = ChatPromptTemplate.from_messages([(\n",
    "    \"system\",\n",
    "    \"You are extracting all names from the text.\",\n",
    "),\n",
    "(\n",
    "    \"human\",\n",
    "    \"Use the given format to extract information from the following input: {question}\",\n",
    "),\n",
    "])\n",
    "\n",
    "entity_chain = prompt | llm.with_structured_output(Entities)\n",
    "# entity_chain.invoke({\"question\": 'What is the school zone of GRACE KING HIGH SCHOOL?'})\n",
    "# entity_chain.invoke({\"question\": 'What are the flood areas of Joshua Butler Elementary School, River Oaks Hospital, Westbank Community School, Paul J. Solis Elementary School, and Stella Worley Middle School?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "import classes\n",
    "from classes import basic, formatted\n",
    "from classes import lambda_basic\n",
    "\n",
    "classes = importlib.reload(classes)\n",
    "\n",
    "def structured_retriever(question: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Collects the neighborhood of entities mentioned\n",
    "    in the question\n",
    "    \"\"\"\n",
    "    entity = entity_chain.invoke({\"question\": question})\n",
    "\n",
    "    # return formatted(entity.names)        \n",
    "    # return basic(entity.names)\n",
    "    return lambda_basic(entity.names)\n",
    "\n",
    "print(structured_retriever(\"What are the flood areas of Joshua Butler Elementary School, River Oaks Hospital, Westbank Community School, Paul J. Solis Elementary School, and Stella Worley Middle School?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "# Condense a chat history and follow-up question into a standalone question\n",
    "_template = \"\"\"\n",
    "Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Follow Up Input: {question}\n",
    "\n",
    "Standalone question:\"\"\"  # noqa: E501\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),  # Condense follow-up question and chat into a standalone_question\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | llm\n",
    "        | StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    RunnableLambda(lambda x : x[\"question\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(question: str):\n",
    "    structured_data = structured_retriever(question)\n",
    "    # unstructured_data = [el.page_content for el in existing_graph.similarity_search(question)]\n",
    "    unstructured_data = []    \n",
    "    final_data = f\"\"\"Structured data:\n",
    "{structured_data}\n",
    "Unstructured data:\n",
    "{\"#Document \". join(unstructured_data)}\n",
    "    \"\"\"\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Use natural language and be concise.\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": _search_query | retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"question\": \"What is the school zone of GRACE KING HIGH SCHOOL?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Basic question using a Neo4j rag chain\n",
    "# \n",
    "\n",
    "questions = [\n",
    "    \"What is the school zone of GRACE KING HIGH SCHOOL?\",\n",
    "    \"What is the flood area of GRACE KING HIGH SCHOOL?\",\n",
    "    \"What school zones have schools in the New Orleans East Bank flood area?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    response = chain.invoke({'question': question, \"chat_history\": [] })\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {response}\")\n",
    "    print()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Chat history approach to get information in multiple steps\n",
    "# \n",
    "\n",
    "questions = [\n",
    "    \"List five schools in the school zone Jefferson Parish School District?\",\n",
    "    \"What are the flood areas of those schools?\"\n",
    "]\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    response = chain.invoke({'question': question, \"chat_history\": chat_history })\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {response}\")\n",
    "    print()\n",
    "    \n",
    "    chat_history.append((question, response));\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "\n",
    "class Neo4jRetriever(BaseRetriever):\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Sync implementations for retriever.\"\"\"\n",
    "        return structured_retriever(query)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Neo4j structured query tool definition\n",
    "# \n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, List\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "class LocationInformation(BaseModel):\n",
    "    name: str = Field(description=\"Name of the location to get information about\")\n",
    "\n",
    "\n",
    "@tool(\"Intermediate Answer\", args_schema=LocationInformation, return_direct=True)\n",
    "def location_search(name: str) -> List[Document]:\n",
    "    \"\"\"Get locations with are associated with a given location.\"\"\"\n",
    "    return Neo4jRetriever().invoke(name)\n",
    "\n",
    "\n",
    "# Let's inspect some of the attributes associated with the tool.\n",
    "print(location_search.name)\n",
    "print(location_search.description)\n",
    "print(location_search.args)\n",
    "print(location_search.return_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_search.invoke({'name': 'Jefferson Parish School District'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Planner approach\n",
    "# \n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class ReWOO(TypedDict):\n",
    "    task: str\n",
    "    plan_string: str\n",
    "    steps: List\n",
    "    results: dict\n",
    "    result: str\n",
    "    \n",
    "prompt = \"\"\"For the following task, make plans that can solve the problem step by step. For each plan, indicate \\\n",
    "which external tool together with tool input to retrieve evidence. You can store the evidence into a \\\n",
    "variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...)\n",
    "\n",
    "Tools can be one of the following:\n",
    "(1) SZA[input]: Worker that searches school zones from graph for associated school information. Useful when you need to find what\n",
    "schools are within a school zone. The input should be a the name of a school zone or a school.\n",
    "(2) FAA[input]: Worker that searches flood areas from graph for associated school information. Useful when you need to find what\n",
    "schools are within a flood area. The input should be a the name of a flood area or a school.\n",
    "(3) LLM[input]: A pretrained LLM like yourself. Useful when you need to act with general\n",
    "world knowledge and common sense. Prioritize it when you are confident in solving the problem\n",
    "yourself. Input can be any instruction.\n",
    "\n",
    "For example,\n",
    "Task: What flood areas are connected to the school zone Jefferson Parish School District?\n",
    "Plan: Given school zones are associated with schools, find which schools are within the Jefferson Parish School District.\n",
    "      #E1 = SZA[Jefferson Parish School District]\n",
    "Plan: List the schools from the school names. \n",
    "      #E2 = LLM[List names given #E1]\n",
    "Plan: Given flood areas are associated with schools, find which flood areas are associated with a school give #E2.\n",
    "      #E3 = FAA[#E2]\n",
    "Plan: List the flood area names. \n",
    "      #E4 = LLM[List flood area names given #E3]\n",
    "\n",
    "Begin! \n",
    "Describe your plans with rich details. Each Plan should be followed by only one #E.\n",
    "\n",
    "Task: {task}\"\"\"\n",
    "\n",
    "task = \"What flood areas are connected to the school zone Jefferson Parish School District?\"\n",
    "\n",
    "result = llm.invoke(prompt.format(task=task))\n",
    "\n",
    "print(result.content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Regex to match expressions of the form E#... = ...[...]\n",
    "regex_pattern = r\"Plan:\\s*(.+)\\s*(#E\\d+)\\s*=\\s*(\\w+)\\s*\\[([^\\]]+)\\]\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"user\", prompt)])\n",
    "planner = prompt_template | llm\n",
    "\n",
    "\n",
    "def get_plan(state: ReWOO):\n",
    "    task = state[\"task\"]\n",
    "    result = planner.invoke({\"task\": task})\n",
    "    # Find all matches in the sample text\n",
    "    matches = re.findall(regex_pattern, result.content)\n",
    "    matches = matches[:len(matches)//2]\n",
    "    \n",
    "    return {\"steps\": matches, \"plan_string\": result.content}\n",
    "\n",
    "def _get_current_task(state: ReWOO):\n",
    "    if \"results\" not in state or state[\"results\"] is None:\n",
    "        return 1\n",
    "    if len(state[\"results\"]) == len(state[\"steps\"]):\n",
    "        return None\n",
    "    else:\n",
    "        return len(state[\"results\"]) + 1\n",
    "\n",
    "\n",
    "def tool_execution(state: ReWOO):\n",
    "    \"\"\"Worker node that executes the tools of a given plan.\"\"\"\n",
    "    _step = _get_current_task(state)\n",
    "    _, step_name, tool, tool_input = state[\"steps\"][_step - 1]\n",
    "    _results = (state[\"results\"] or {}) if \"results\" in state else {}\n",
    "    for k, v in _results.items():\n",
    "        tool_input = tool_input.replace(k, v)\n",
    "    if tool == \"SZA\":\n",
    "        print('SZA Tool input: ' + str(tool_input))\n",
    "        result = Neo4jRetriever().invoke(tool_input)\n",
    "    elif tool == \"FAA\":\n",
    "        tool_pattern = r\"content='(.*)' additional_kwargs=.*\"\n",
    "        tool_matches = re.search(tool_pattern, tool_input)\n",
    "\n",
    "        print('FAA Tool input: ' + tool_matches.group(1))        \n",
    "        result = Neo4jRetriever().invoke(tool_matches.group(1))\n",
    "    elif tool == \"LLM\":\n",
    "        result = llm.invoke(tool_input)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    _results[step_name] = str(result)\n",
    "    return {\"results\": _results}\n",
    "\n",
    "solve_prompt = \"\"\"Solve the following task or problem. To solve the problem, we have made step-by-step Plan and \\\n",
    "retrieved corresponding Evidence to each Plan. Use them with caution since long evidence might \\\n",
    "contain irrelevant information.\n",
    "\n",
    "{plan}\n",
    "\n",
    "Now solve the question or task according to provided Evidence above. Respond with the answer\n",
    "directly with no extra words.\n",
    "\n",
    "Task: {task}\n",
    "Response:\"\"\"\n",
    "\n",
    "\n",
    "def solve(state: ReWOO):\n",
    "    plan = \"\"\n",
    "    for _plan, step_name, tool, tool_input in state[\"steps\"]:\n",
    "        _results = (state[\"results\"] or {}) if \"results\" in state else {}\n",
    "        for k, v in _results.items():\n",
    "            tool_input = tool_input.replace(k, v)\n",
    "            step_name = step_name.replace(k, v)\n",
    "        plan += f\"Plan: {_plan}\\n{step_name} = {tool}[{tool_input}]\"\n",
    "    prompt = solve_prompt.format(plan=plan, task=state[\"task\"])\n",
    "    result = llm.invoke(prompt)\n",
    "    return {\"result\": result.content}\n",
    "\n",
    "def _route(state):\n",
    "    _step = _get_current_task(state)\n",
    "    if _step is None:\n",
    "        # We have executed all tasks\n",
    "        return \"solve\"\n",
    "    else:\n",
    "        # We are still executing tasks, loop back to the \"tool\" node\n",
    "        return \"tool\"\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "graph = StateGraph(ReWOO)\n",
    "graph.add_node(\"plan\", get_plan)\n",
    "graph.add_node(\"tool\", tool_execution)\n",
    "graph.add_node(\"solve\", solve)\n",
    "graph.add_edge(\"plan\", \"tool\")\n",
    "graph.add_edge(\"solve\", END)\n",
    "graph.add_conditional_edges(\"tool\", _route)\n",
    "graph.add_edge(START, \"plan\")\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "for s in app.stream({\"task\": task}):\n",
    "    print(s)\n",
    "    print(\"---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# ReAct loop approach\n",
    "# \n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, AgentOutputParser\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, List\n",
    "from langchain_core.tools import tool\n",
    "from typing import List, Union\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import AgentAction, AgentFinish, HumanMessage, SystemMessage, BaseMessage\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver  # an in-memory checkpointer\n",
    "import operator\n",
    "\n",
    "\n",
    "# Defining state\n",
    "class GraphState(TypedDict):\n",
    "   input: str\n",
    "   chat_history: list[BaseMessage]\n",
    "   intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n",
    "   \n",
    "   \n",
    "class LocationSearch(BaseModel):\n",
    "    name: str = Field(description=\"Name of the location to get information about\")\n",
    "\n",
    "@tool(\"LOC_SEARCH\", args_schema=LocationSearch, return_direct=True)\n",
    "def search(name: str) -> str:\n",
    "    \"\"\"Get locations with are associated with a given location.\"\"\"\n",
    "    if name.index('School District') != -1:\n",
    "        return \"The following schools are part of Jefferson Parish School District: JOSHUA BUTLER ELEMENTARY SCHOOL, RIVER OAKS HOSPITAL, WESTBANK COMMUNITY SCHOOL, PAUL J. SOLIS ELEMENTARY SCHOOL, WEST JEFFERSON HIGH SCHOOL, BONELLA A. ST. VILLE ELEMENTARY SCHOOL, STELLA WORLEY MIDDLE SCHOOL, CONGETTA TRIPPE JANET ELEMENTARY SCHOOL, JEFFERSON ELEMENTARY SCHOOL, HARRY S. TRUMAN MIDDLE SCHOOL, L.H. MARRERO MIDDLE SCHOOL, RIVERSIDE ALTERNATIVE HIGH SCHOOL, VIC A. PITRE ELEMENTARY SCHOOL, RUPPEL ACADEMIE FRANCAISE, J.C. ELLIS ELEMENTARY SCHOOL.\"\n",
    "    \n",
    "    # return (', '.join(map(lambda doc: doc.page_content, Neo4jRetriever().invoke(name) ))) \n",
    "    return \"The following schools are in the flood area New Orleans West Bank: JOSHUA BUTLER ELEMENTARY SCHOOL, WESTBANK COMMUNITY SCHOOL, STELLA WORLEY MIDDLE SCHOOL. The following schools are in the flood area New Orleans East Bank: RIVER OAKS HOSPITAL\"\n",
    "   \n",
    "\n",
    "@tool(\"final_answer\")\n",
    "def final_answer(\n",
    "   locations: list[str]\n",
    "):\n",
    "   \"\"\"Returns a natural language response to the user as list of the locations\n",
    "   \"\"\"\n",
    "   if type(locations) is list:\n",
    "       locations = \"\\n\".join([f\"- {r}\" for r in locations])\n",
    "   return \"\"\n",
    "\n",
    "tools=[\n",
    "   search,\n",
    "   final_answer\n",
    "]\n",
    "\n",
    "tool_str_to_func = {\n",
    "   \"LOC_SEARCH\": search,\n",
    "   'final_answer': final_answer   \n",
    "}\n",
    "\n",
    "def create_scratchpad(intermediate_steps: list[AgentAction]):\n",
    "   research_steps = []\n",
    "   for i, action in enumerate(intermediate_steps):\n",
    "       if action.log != \"TBD\":\n",
    "           # this was the ToolExecution\n",
    "           research_steps.append(\n",
    "               f\"Tool: {action.tool}, input: {action.tool_input}\\n\"\n",
    "               f\"Output: {action.log}\"\n",
    "           )\n",
    "   return \"\\n---\\n\".join(research_steps)\n",
    "\n",
    "system_prompt = \"\"\"You are the oracle, the great AI decision maker.\n",
    "Given the user's query you must decide what to do with it based on the\n",
    "list of tools provided to you.\n",
    "\n",
    "\n",
    "If you see that a tool has been used (in the scratchpad) with a particular\n",
    "query, do NOT use that same tool with the same query again. Also, do NOT use\n",
    "any tool more than twice (ie, if the tool appears in the scratchpad twice, do\n",
    "not use it again).\n",
    "\n",
    "\n",
    "You should aim to collect information from a diverse range of sources before\n",
    "providing the answer to the user. Once you have collected plenty of information\n",
    "to answer the user's question (stored in the scratchpad) use the final_answer\n",
    "tool.\n",
    "\n",
    "\n",
    "The data is structured such that flood areas are only associated with schools.  \n",
    "School zones are only associated with schools. In order to get the flood area\n",
    "of a school zone, you must find the schools assocaited with that school zone\n",
    "and then find the flood areas associated with those schools.\n",
    "\n",
    "\n",
    "Tools can be one of the following:\n",
    "(1) LOC_SEARCH[input]: Worker that searches locations and returns a list of associated locations.\n",
    "(2) final_answer[input]: Formats the final answer\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "   (\"system\", system_prompt),\n",
    "   MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "   (\"user\", \"{input}\"),\n",
    "   (\"assistant\", \"scratchpad: {scratchpad}\")\n",
    "])\n",
    "\n",
    "llm_with_tools = ChatOllama(model=MODEL, temperature=0).bind_tools(tools)\n",
    "\n",
    "oracle = (\n",
    "   {\n",
    "       \"input\": lambda x: x[\"input\"],\n",
    "       \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "       \"scratchpad\": lambda x: create_scratchpad(\n",
    "           intermediate_steps=x[\"intermediate_steps\"]\n",
    "       ),\n",
    "   }\n",
    "   | prompt\n",
    "   | llm_with_tools\n",
    ")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "def run_oracle(state: GraphState) -> SystemMessage:\n",
    "   global counter\n",
    "#    print(f\"run oracle state: {state}\")   \n",
    "   print(state)\n",
    "   \n",
    "   # This is where the reasoning happens\n",
    "   out =  oracle.invoke(state)\n",
    "   \n",
    "   print(out)\n",
    "   \n",
    "#    tool_name = out.tool_calls[0][\"name\"]\n",
    "#    tool_args = out.tool_calls[0][\"args\"]\n",
    "   \n",
    "#    action_out = AgentAction(\n",
    "#        tool=tool_name,\n",
    "#        tool_input=tool_args,\n",
    "#        log=\"TBD\"\n",
    "#    )\n",
    "\n",
    "   action_out = AgentAction(\n",
    "       tool=\"LOC_SEARCH\",\n",
    "       tool_input=\"Jefferson Parish School District\" if counter == 0 else \"JOSHUA BUTLER ELEMENTARY SCHOOL\",\n",
    "       log='TBD'\n",
    "   )\n",
    "   \n",
    "   counter += 1\n",
    "   \n",
    "   return {\n",
    "       \"intermediate_steps\": [action_out]\n",
    "   }\n",
    "\n",
    "def router(state: GraphState):\n",
    "   # return the tool name to use\n",
    "   if isinstance(state[\"intermediate_steps\"], list):\n",
    "       return state[\"intermediate_steps\"][-1].tool\n",
    "   else:\n",
    "       # if we output bad format go to final answer\n",
    "       print(\"Router invalid format\")\n",
    "       return \"final_answer\"\n",
    "\n",
    "def run_tool(state: GraphState):\n",
    "   # use this as helper function so we repeat less code\n",
    "   tool_name = state[\"intermediate_steps\"][-1].tool\n",
    "   tool_args = state[\"intermediate_steps\"][-1].tool_input\n",
    "   \n",
    "   print(f\"Executing tool: {tool_name}.invoke(input={tool_args})\")\n",
    "      \n",
    "   # run tool\n",
    "   out = tool_str_to_func[tool_name].invoke(input=tool_args)\n",
    "   action_out = AgentAction(\n",
    "       tool=tool_name,\n",
    "       tool_input=tool_args,\n",
    "       log=str(out)\n",
    "   )\n",
    "   \n",
    "   return {\"intermediate_steps\": [action_out]}\n",
    "   \n",
    "\n",
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        print(s)\n",
    "        # messages = s[\"chat_history\"]        \n",
    "        # message = messages[-1] if messages else \"\"        \n",
    "        \n",
    "        # if isinstance(message, tuple):\n",
    "        #     print(message)\n",
    "        # elif isinstance(message, BaseMessage):\n",
    "        #     message.pretty_print()\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "# initialize the graph with our AgentState\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# add nodes\n",
    "workflow.add_node(\"oracle\", run_oracle)\n",
    "workflow.add_node(\"LOC_SEARCH\", run_tool)\n",
    "workflow.add_node(\"final_answer\", run_tool)\n",
    "\n",
    "# specify the entry node\n",
    "workflow.set_entry_point(\"oracle\")\n",
    "\n",
    "\n",
    "# add the conditional edges which use the router\n",
    "workflow.add_conditional_edges(\n",
    "   source=\"oracle\",  # where in graph to start\n",
    "   path=router,  # function to determine which node is called\n",
    ")\n",
    "\n",
    "\n",
    "# create edges from each tool back to the oracle\n",
    "for tool_obj in tools:\n",
    "   if tool_obj.name != \"final_answer\":\n",
    "       workflow.add_edge(tool_obj.name, \"oracle\")\n",
    "\n",
    "# if anything goes to final answer, it must then move to END\n",
    "workflow.add_edge(\"final_answer\", END)\n",
    "\n",
    "\n",
    "# finally, we compile our graph\n",
    "graph = workflow.compile()\n",
    "config = {\"configurable\": {\"thread_id\": \"test-thread\"}}\n",
    "\n",
    "inputs = {\n",
    "    \"input\": \"What flood areas are connected to schools in the school zone Jefferson Parish School District?\",\n",
    "    \"chat_history\": [],\n",
    "    \"intermediate_steps\": [],\n",
    "}\n",
    "\n",
    "# out = llm_with_tools.invoke(\"What flood areas are connected to schools in the school zone Jefferson Parish School District?\")\n",
    "# print(out)\n",
    "# print(out.tool_calls)\n",
    "\n",
    "# out = (prompt | llm_with_tools).invoke({\n",
    "#     \"input\": \"What flood areas are connected to schools in the school zone Jefferson Parish School District?\",\n",
    "#     \"chat_history\": [],\n",
    "#     \"intermediate_steps\": [],\n",
    "#     \"scratchpad\": \"\"\n",
    "# })\n",
    "# print(out)\n",
    "# print(out.tool_calls)\n",
    "\n",
    "\n",
    "# out = oracle.invoke(inputs)\n",
    "# print(out)\n",
    "# out.tool_calls\n",
    "            \n",
    "# print_stream(graph.stream(inputs, config, stream_mode=\"values\"))\n",
    "graph.invoke(inputs, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
